{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WN-mPNRag4x_a5iXHiZA2Mz03XiHcL2c",
      "authorship_tag": "ABX9TyNFiIr89KT/kcQpjww3eUah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vetu2910/PCA/blob/main/coviddetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdbFx1IZLfEr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5s8klFdLxHx"
      },
      "source": [
        "IMPORTING THE DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJiHveJrIgza"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Conv2D,Dense,MaxPool2D,Dropout,Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-_VdLkkyuso"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf71GRDyMxSZ"
      },
      "source": [
        "Image augmentation to improve the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0gL_CRwL1ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5133efc-1399-4d16-88b9-c1ee73feeb19"
      },
      "source": [
        "train_datagen = image.ImageDataGenerator(\n",
        "    rescale = 1/255,horizontal_flip = True,zoom_range = 0.2,shear_range = 0.2\n",
        ")\n",
        "train_data = train_datagen.flow_from_directory(directory = \"/content/drive/MyDrive/project dataset complete\",target_size = (256,256),batch_size = 16,class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13839 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njZrLQxC1vOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe0d953-0084-4d4c-d2c2-288d912d05e7"
      },
      "source": [
        "train_data.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'covid': 0, 'normal': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_-xJJu65_aD",
        "outputId": "8cd3f55f-9ec1-4a61-a905-ce2f0e910396"
      },
      "source": [
        "test_datagen = image.ImageDataGenerator(\n",
        "    rescale = 1/255\n",
        ")\n",
        "test_data = train_datagen.flow_from_directory(directory = \"/content/drive/MyDrive/project test\",target_size = (256,256),batch_size = 16,class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 72 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_-PFBPsNibc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6wuNPfUH8oK",
        "outputId": "f8f84a72-f4de-4b47-9849-2c63c409d506"
      },
      "source": [
        "test_data.class_indices\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'covid': 0, 'normal': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS7Fe6cYdZcg"
      },
      "source": [
        "BUILDING OUR COVID MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqhr-TvjIAZa"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32,kernel_size = (3,3),activation = 'relu',input_shape = (256,256,3)))\n",
        "\n",
        "model.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Dropout(rate = 0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Dropout(rate = 0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 128,kernel_size = (3,3),activation = 'relu'))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Dropout(rate = 0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64,activation = 'relu'))\n",
        "model.add(Dropout(rate = 0.15))\n",
        "\n",
        "model.add(Dense(units = 1,activation= 'softmax'))\n",
        "\n",
        "model.compile(loss = keras.losses.BinaryCrossentropy(),optimizer = 'adam',metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lriw8BJqLXL9",
        "outputId": "960ce1a3-8f72-4ae7-91bc-aede6c1257e6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 252, 252, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 126, 126, 64)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 126, 126, 64)      0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 124, 124, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 62, 62, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 115200)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                7372864   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,503,105\n",
            "Trainable params: 7,503,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjDaPpKSLXPU",
        "outputId": "97db0220-a28d-4dda-f8f3-5b4936bc11fd"
      },
      "source": [
        "model.fit(train_data,# batch_size = 512,\n",
        "          steps_per_epoch = 15  ,epochs = 50   # ,validation_split=0.2\n",
        "                #   validation_data = test_data\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 79s 3s/step - loss: 0.7730 - acc: 0.7167\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 48s 3s/step - loss: 0.6201 - acc: 0.7167\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 51s 3s/step - loss: 0.6410 - acc: 0.6792\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 48s 3s/step - loss: 0.5992 - acc: 0.7333\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 50s 3s/step - loss: 0.5957 - acc: 0.7322\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 46s 3s/step - loss: 0.5596 - acc: 0.7792\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 51s 3s/step - loss: 0.6022 - acc: 0.7000\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.5740 - acc: 0.7583\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 46s 3s/step - loss: 0.5485 - acc: 0.7500\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 46s 3s/step - loss: 0.5265 - acc: 0.7583\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 45s 3s/step - loss: 0.5019 - acc: 0.7417\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.5332 - acc: 0.7458\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 39s 3s/step - loss: 0.5247 - acc: 0.7500\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.5159 - acc: 0.7250\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 38s 3s/step - loss: 0.5388 - acc: 0.7573\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 36s 2s/step - loss: 0.4961 - acc: 0.7375\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 39s 3s/step - loss: 0.4517 - acc: 0.7750\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 39s 3s/step - loss: 0.5223 - acc: 0.7542\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 37s 2s/step - loss: 0.5186 - acc: 0.7583\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 38s 3s/step - loss: 0.4822 - acc: 0.7667\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 39s 3s/step - loss: 0.4766 - acc: 0.7917\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 42s 3s/step - loss: 0.4753 - acc: 0.7667\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 37s 2s/step - loss: 0.4708 - acc: 0.7917\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 38s 2s/step - loss: 0.4139 - acc: 0.7875\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.5072 - acc: 0.7667\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 36s 2s/step - loss: 0.5111 - acc: 0.7333\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.4805 - acc: 0.7875\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.4722 - acc: 0.7750\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.4750 - acc: 0.7750\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.4749 - acc: 0.7708\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.4974 - acc: 0.7750\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.3951 - acc: 0.8292\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 33s 2s/step - loss: 0.5735 - acc: 0.7458\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.4677 - acc: 0.7708\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.5164 - acc: 0.7417\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 36s 2s/step - loss: 0.5299 - acc: 0.7208\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.4462 - acc: 0.7917\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.3764 - acc: 0.8500\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.5249 - acc: 0.7458\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.4895 - acc: 0.7583\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 25s 2s/step - loss: 0.4452 - acc: 0.8042\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 33s 2s/step - loss: 0.4960 - acc: 0.7542\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 28s 2s/step - loss: 0.4883 - acc: 0.7625\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 23s 2s/step - loss: 0.5053 - acc: 0.7500\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 23s 2s/step - loss: 0.5071 - acc: 0.7208\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 26s 2s/step - loss: 0.4588 - acc: 0.8033\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 26s 2s/step - loss: 0.4391 - acc: 0.7792\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 24s 2s/step - loss: 0.4496 - acc: 0.7833\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 26s 2s/step - loss: 0.4476 - acc: 0.7625\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 21s 1s/step - loss: 0.3932 - acc: 0.8083\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd494a3490>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdNVz7JhPZ1u"
      },
      "source": [
        "# model.fit(test_data ,steps_per_epoch = 2,epochs = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqSK_v1AKaBw"
      },
      "source": [
        "model.compile(\n",
        "    optimizer= tf.keras.optimizers.RMSprop(),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC0u53NjEeRK",
        "outputId": "f6e483e6-c8fd-4eeb-e9a7-c75e697eaed0"
      },
      "source": [
        "results = model.evaluate(test_data,batch_size = 2)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 14s 3s/step - loss: nan - sparse_categorical_accuracy: 0.5833\n",
            "test loss, test acc: [nan, 0.5833333134651184]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHOM-vy6SLcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce38548a-eb4b-48de-d5f1-f5c4aa0c8f98"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Chest.jpeg\"\n",
        "img = image.load_img(path,target_size=(256,256))\n",
        "\n",
        "img = image.img_to_array(img)/255\n",
        "img = np.array([img])\n",
        "# tf.image.resize(img, [1,256,256,3]).shape.as_list()\n",
        "img.shape\n",
        "# img = image.reshape(1,256,256,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvnexb0BO6-y",
        "outputId": "52cb2995-f961-4391-b468-156959a321b5"
      },
      "source": [
        "model.predict(img)\n",
        "#(model.predict(img) > 0).astype(\"int32\")\n",
        "predictions = (model.predict(img) < 0.6).astype(\"int32\")\n",
        "predictions\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4PqiUWKZkZW"
      },
      "source": [
        "class 0 --> covid affected\n",
        "class 1 --> normal lung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XFrGsEvVVRz",
        "outputId": "1ce51108-06b1-49cf-df04-d61f077855c9"
      },
      "source": [
        "model.predict(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7374685]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRzY-czKVd-T",
        "outputId": "3437f1d5-dd89-4e4d-86cf-0e9acf698e7c"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Normal.png\"\n",
        "img = image.load_img(path,target_size=(256,256))\n",
        "\n",
        "img = image.img_to_array(img)/255\n",
        "img = np.array([img])\n",
        "# tf.image.resize(img, [1,256,256,3]).shape.as_list()\n",
        "img.shape\n",
        "# img = image.reshape(1,256,256,3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0t_Z5QgWO_a",
        "outputId": "3ab7ea4e-8837-49ca-9ccd-d97cf5eb69d5"
      },
      "source": [
        "# y = keras.np_utils.probas_to_classes(img)\n",
        "#model.predict_classes(img)\n",
        "(model.predict(img) > 0.6).astype(\"int32\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}